FROM ubuntu:bionic

ENV SPARK_VERSION=3.0.2
ENV HADOOP_VERSION=2.7
ENV JAVA_HOME /usr/lib/jvm/java-1.8.0-openjdk-amd64/jre
ENV HADOOP_HOME /opt/hadoop
ENV HADOOP_CONF_DIR /opt/hadoop/etc/hadoop
ENV SPARK_HOME /opt/spark
ENV PATH="${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${PATH}"

# install java and python
RUN apt-get update && \
    apt-get install -y wget nano openjdk-8-jdk ssh openssh-server
RUN apt update && apt install -y python3 python3-pip python3-dev build-essential libssl-dev libffi-dev libpq-dev

# install python packages
COPY /confs/requirements.req /
RUN pip3 install -r requirements.req
RUN pip3 install dask[bag] --upgrade
RUN pip3 install --upgrade toree
RUN python3 -m bash_kernel.install

# install HDFS
RUN wget -P /tmp/ https://archive.apache.org/dist/hadoop/common/hadoop-2.7.0/hadoop-2.7.0.tar.gz
RUN tar xvf /tmp/hadoop-2.7.0.tar.gz -C /tmp && \
	mv /tmp/hadoop-2.7.0 /opt/hadoop

#install spark
RUN wget -P /tmp/ https://downloads.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz
RUN tar xvf /tmp/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /tmp && \
    mv /tmp/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} ${SPARK_HOME}

# generate an SSH key to allow communication between our containers
RUN ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
	cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
	chmod 600 ~/.ssh/authorized_keys
COPY /confs/config /root/.ssh
RUN chmod 600 /root/.ssh/config

# copy configuration files for hadoop and spark
COPY /confs/*.xml /opt/hadoop/etc/hadoop/
COPY /confs/slaves /opt/hadoop/etc/hadoop/
COPY /script_files/bootstrap.sh /
COPY /confs/spark-defaults.conf ${SPARK_HOME}/conf

# install pyspark
RUN pip3 install pyspark
ENV PYSPARK_PYTHON "/usr/bin/python3"
ENV PATH "$PATH:$SPARK_HOME/bin"
ENV PYTHONPATH "$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9-src.zip:$PYTHONPATH"

# set environment variables 
RUN echo "export JAVA_HOME=${JAVA_HOME}" >> /etc/environment

# Exposing ports so the container can listen to them
EXPOSE 9000
EXPOSE 7077
EXPOSE 4040
EXPOSE 8020
EXPOSE 22

# copy log processing code
#WORKDIR /root/
COPY log-processing/mi.tgz /root/mi.tgz
RUN tar xzvf /root/mi.tgz -C /root/
ENV PYTHONPATH "/root/:$PYTHONPATH"
RUN apt update && apt-get install -y netcat

COPY log-processing/decoder_v2.py /root/
COPY log-processing/logCounter.py /root/
COPY log-processing/logTypeCounter.py /root/
#COPY log-processing/LogReceiver.java /root/
COPY log-processing/sample_log /root/
#RUN javac /root/LogReceiver.java
COPY log-processing/logReceiver.sh /root/
RUN chmod +x /root/logReceiver.sh


ENTRYPOINT ["/bin/bash", "bootstrap.sh"]
